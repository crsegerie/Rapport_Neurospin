
\chapter{Sensor}

% [metre le détail des methodes ICA si pas assez de maths]

\section{CSP algorithm}

\subsection{CSP running time optimization}
\label{Sec:running_time_optimisation}

The optimization of the computation time does not change the neuroanatomical results presented, and therefore is not integrated in the "method" part, but still required a considerable amount of work.

My first implementation of the CSP algorithm took a whole day to compute for a single subject, which corresponds to a 15 days computation for our cohort. It was therefore crucial to find ways to greatly accelerate the calculations, which will then be reiterated at each commit of the pipeline as part of the continuous integration with Circle CI.

A great deal of research and engineering effort was then put into reducing the calculation time.

\paragraph{Technique d'ingenieurie logicielles classique}

Classic software engineering techniques have been key to massively reduce computation times: factoring of all steps, use of tools such as the line-profiler - which allows to visualize the execution time of each line -  use of multi-processing. On the other hand, even if mathematically some operations are commutative, in practice, the choice of the order of the operations allows to save computation time: for example even if the operations (time crop, frequency filter) are commutative, it is better to start by cropping the data and then filtering, because the filtering operation is very expensive.

\paragraph{Dimensionality reduction}

Once all the software improvements have been made, we can also make improvements coming from machine learning techniques by using dimensionality reduction techniques: We can either select the type of channels, or use a PCA.

\begin{figure}[ht]
    \centering
    \includegraphics[width=5cm]{images_report/sensor/sensors_elekta.png}
    \caption[MEG Sensors]%
    {MEG Sensors [\href{https://www.elekta.com/dam/jcr:ed6d88e7-cd3e-478e-9c4a-3f89ef90ec92/Elekta-Neuromag-TRIUX-Brochure.pdf}{ELekta documentation}]}
    \label{sensors_elekta}
\end{figure}

\begin{itemize}
    \item Reduction of dimensionality by selection of the channel type: The MEG data comes from 102 sensors, which are distributed as shown in the figure \ref{sensors_elekta}. Each sensor consists of 2 gradiometers measuring the gradiometric field (Tesla/meter) in the direction tangent to the sensor plane, and a magnetometer measuring the electric field in the direction normal to the sensor plane. Magnetometers are robust to external noise, while magnetometers are more exposed. But magnetometers have two crucial advantages: 1. they record localized spatial information, which is perfectly suited to the CSP algorithm, which subsequently favors the interpretability of patterns from the CSP. 2. The magnetometers are half as numerous as the gradiometers.Thus, by selecting only the magnetometers, the number of channels is divided by three.  The CSP algorithm being limited by an SVD whose complexity in $\mathcal{O}(T n^2)$ evolves as a function of the square of the number $n$ of channels \cite{dhillonalan}, we thus accelerate the algorithm by a factor 9. (T is the number of time point, in our case bigger than the number of channels. The complexity of the SVD internal to the CSP is in square of the minimum between $T$ and $n$).
    \item The following point applies only to data from a MEG, but not to EEGs. Indeed, EEGs have only one channel tyepe. We can therefore reduce the dimensionality using a simple PCA.
\end{itemize}


\paragraph{Sampling}

The last way to accelerate the computation time is to use sampling, i. e. to reduce the sampling frequency. This is also called decimation. For example $decim = 5$ consists in taking only one point on 5. But sampling can introduce numerical instabilities. Indeed, the CSP algorithm must start by estimating the covariance matrix of the signal. However, the estimation of a covariance matrix of dimension $n$ requires at least approximately $n$ points. With a sampling frequency of 100 Hz, over a time window of 0.5 seconds, we have 50 points. It is therefore necessary to reduce the dimensions to at most 50 or to use covariance regularization methods.

[Nyquist]

\paragraph{CSP running time optimization results}

The choice of the order of commutativity, the reduction of the dimensionality and the resampling allow us today to obtain results in less than three hours with 8Cross validation for all subjects. These three techniques allow us to respectively accelerate by $2*10*5$ on our dataset while keeping similar performances in all points. It even seems that the PCA increases the numerical stability of the results.

% - Interaface avec le reste de la pipeline, openscience, reproductibilité
% - Anexes : Implémentation et choix d'architecte logiciel

\subsection{Iterations levels}

Choisir l'ordre des iterations n'a pas été une mince affaire, bien que ce soit ici plus un probleme d'ingénieurie que de la recherche pure.

- contrasts
- We iterate through subjects and sessions
- If there are multiple runs, runs are concatenated into one session.

\subsection{CSP Regularization}

\subsection{CSP : Variance vs Second order moment}

\subsection{CSP Solving}


\subsection{Subtilités mathématiques}

\subsubsection{Why not aiming for the best classifier?}
Mathetically we do not seek here to create the best classifier,
and to optimize the rocauc score, we only seek
to obtain unbiased scores usable afterwards in the permutation test.
This is why it is not a big deal to optimize the running time
by making some approximations, such as:
- Decimation
- Selection of the mag channels, which contain a very large part of
the information contained in (mag+grad)
- PCA, which is very useful to reduce the dimension for eeg
where there is only one channel type.


\subsubsection{philosophie de pourquoi c'est ok de réduire un peu les performances, Pourquoi ne pas se battre pour les performances avec les csp.}

By thinking more about it, It feels to me that this group cross validation is not even necessary at all for mathematical rigor. Because here we just want to construct a statistic for each frequency bin. Here our statistic is the size of the cluster after thresholding the roc auc score coming from a usual cross validation. But our statistic could really be anything. The permutation test is just here to confirm that "There is a difference", whatever the difference is. So it's perfectly valid to use any metric : accuracy is as legitimate as roc auc and our cross validation is as legitimate as a group cross validation.
Another way to say it is that : we do not want to test the generalization ability of our classifier between different runs. We just want to say : Our classifier proves a difference exists between the two conditions.
So I think no need to worry because no need for group cross-validation.


\subsubsection{Possiblité de mettre un des frequences et des crop non uniformes}

\subsubsection{choix du seuil de la t-valeur}
En choisissant un seuil de t-valeur de $0.01$ on obtient la figure \ref{permutation_statistics_results}, avec un cluster ayant une p-valuer de $10^{-3.6}$. J'ai choisit ce seuil restrictif de 0.01 pour mon image finale afin que limiter l'aire de mon cluster et afin de se limiter aux zones les plus significatives. Mais il est possible en choisissant un seuil de de $0.05$ d'obtenir une figure avec un cluster allant parcourant toute la bande alpha de 0 à 5 secondes.

\subsubsection{topographic map}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.55\textwidth}
       \includegraphics[width=1\linewidth]{images_report/sensor/csp_individual/sub_155_alpha.png}
       \caption{Example of CSP components (alpha band)}
       \label{fig:csp_component_alpha_band}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.55\textwidth}
       \includegraphics[width=1\linewidth]{images_report/sensor/csp_individual/sub_215_beta.png}
       \caption{Example of CSP components (beta band)}
       \label{fig:csp_component_beta_band}
    \end{subfigure}

    \caption[Example of csp components from the alpha and beta band.]%
    {Example of csp components from the alpha and beta band.}
    \label{example_csp_component}
\end{figure}

Expliquer comment lire les topographic map., regle de la main droite.
Les patterns des csp sont du machine learning ultra interpretable.


\section{Permutations statistics algorithm}

\subsection{t-values calculation}

Les t-valeur est un test parametrique qui suppose la gaussianité des valuer sours jacentes caluclé. Cette hypothése de gaussianité n'es past tojours vrifiée dpour des données venant d'imagerie cerebrale en raison des nombreux filtres préliminaires. Mais dans notre cas, on calucle les t-valeur à partir de la différence entre le roc-auc score et le niveau de chance moyen ce qui permet de rétablir l'hypothese de gaussianité.

Le calcul des p-valeurs se fait pour chaque time-frequency bin de manière indépendante.

On considère la liste des différences $(X_i)_{i \in [\text{Nb Subjects}]}$ pour tous les sujets entre le roc-auc et le chance level $c=0.5$ pour une time frequency bin.

On soustrait alors toutes ces cartes au chance level d'une roc-auc à savoir $0.5$. On obtient alors la différence entre le roc-auc et le chance level.

Il suffit alors de Calculate the T-test for the mean of ONE group of scores.

This is a test for the null hypothesis that the expected value (mean) of a sample of independent observations a is equal to the given population mean, popmean.

Ce test permet de rejeter l'hypothese nulle sous laquelle la moyenne d'une population d'observation independantes est egal à $0$.

Formellement,
On veut comparer la moyenne $\mu$ d'une population de loi normale et d’écart type $\sigma$ non connu à $0$. Pour ce faire, on calcule la moyenne empirique $\overline{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$ et l'estimateur  sans biais $S^{\ast ^2}_n$ de sa variance $\sigma^2$
:$S^{\ast ^2}_n = \frac{1}{n-1}\sum\limits_{i=1}^n (X_i - \overline X_n )^2$.

Selon l’hypothese nulle, la distribution d’échantillonnage de cette moyenne se distribue elle aussi normalement avec un écart type $\frac{\sigma}{\sqrt{n}}$.

La statistique de test:

$ Z = \sqrt{n}\frac{\overline{X} - \mu_0}{S^{\ast}_n}$
suit alors une [[loi de Student]] à $n-1$ degrés de liberté sous l'hypothèse nulle (c'est le théorème de Cochran).

On choisit un risque $\alpha$, généralement $0.05$ ou $0.01$ et l'on calcule la réalisation de la statistique de test :

:$z = \sqrt{n}\frac{\overline{x}_n - \mu_0}{s^{\ast}_n},$ où $s^{\ast}_n =\sqrt{\frac{1}{n-1}\sum\limits_{i=1}^n (x_i - \overline x_n )^2} $

% Si l'on veut tester H0 : μ ≥ μ0 :
% Si z est inférieur au quantile d'ordre α de la loi de Student à n – 1 degrés de liberté alors on rejette l'hypothèse nulle.

% Implémentation se fait à l'aide la fonction scipy.stats.ttest_1samp.


\subsection{Limites algorithmiques}
[annex]

- Choix du nombre de Bin est quelque peu aritraire. Il fut juste respecter quelques conditions telle que Nyquist, plus le fait d'avoir assez de point pour estimer la matrice de covariance. Mais ici, ce n'est pas gracve d'ête arbitraire sur le nombre de point étant donné que le résultat du snesor space n'est utilisé que pour cibler le travail en allant dans le source step.
- Le fait de pouvoir choisir la taille des cluster simplement en ajustant le threshold de la t-value est perturant à première vue. Mais en réalité c'est plutot un bon signe. Nous n'avons testé que deux t-velur différentes (0.01 er 0.05) et le fait d'obtenir deux cluster de taille différentes signiifie simplement que notre cluster associé à 0.05 présente une différence significative tout comme notre cluster à 0.01 qui est assui significative. L'un n'exclue par l'autre.


\chapter{Source}

- Annexes : Subtilité :
Neuroscience : quest ce qui constitue du bruit ? Quel choix de la matrice de covariance
Subtilités mathématiques :  commutativité Log + moyenne
BEM model freeview



\begin{figure}[ht]
    \centering
    \includegraphics[width=15cm]{images_report/source/BEM_model_freeview_cropped.png}
    \caption[Segemntentation results visualized on Freeview.]%
    {BEM model used. Only one because MEG transparent.}
    \label{BEM_model_freeview_cropped}
\end{figure}
